---
title: "Lec08 Intro to Machine Learning"
author: "Michael Nguyen"
date: "10/24/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means clustering

```{r}
# Generate some example data for clustering 
tmp <- c(rnorm(30, -3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp)) 

plot (x)
```

Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results
```{r}
k <- kmeans(x, centers = 2, nstart = 20)
k
```

Q. How many points are in each cluster?
 ->30 each
Q. What ‘component’ of your result object details
- cluster size?
```{r}
k$size
```

- cluster assignment/membership? 
```{r}
k$cluster
```

- cluster center?
```{r}
k$centers
```

Plot x colored by the kmeans cluster assignment and
      add cluster centers as blue points
```{r}
plot(x, col = k$cluster)
points(k$centers, col = "blue", pch = 15)
```



## Heirarchal clustering in R
The `hclust()` function requires a distance matrix as input. You can get this from the `dist()` function
```{r}
# First we need to calculate point (dis)similarity # as the Euclidean distance between observations 
dist_matrix <- dist(x)

# The hclust() function returns a hierarchical # clustering model
hc <- hclust(d = dist_matrix)

# the print method is not so useful here
hc
```

```{r}
plot(hc)
# There are 2 clusters. numbers < 30 on L; numbers > 30 on R

abline(h = 6, col="red")
# cut by height h
cutree(hc, h = 6)

# can save cutree() data into variabel (grps for ex)
```

```{r}
cutree(hc, k =4)
```

### Can cut by HEIGHT h or k


Practice
```{r}
# Step 1. Generate some example data for clustering
y <- rbind(
matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1 
matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2 
matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
         rnorm(50, mean = 0, sd = 0.3)), ncol = 2)) 
colnames(y) <- c("x", "y")

# Step 2. Plot the data without clustering
plot(y)

# Step 3. Generate colors for known clusters (just so we can compare to hclust results) 

col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(y, col=col)
```

### kmeans(x, centers, nstart); hclust(dist(x))
```{r}
# K-means
kmeans(y, centers = 3, nstart = 20)
# heirarchal clustering
hclust(dist(y))
```

Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters

```{r}
# Clustering
hc2 <- hclust(dist(y))

# Draw Tree
plot(hc2)
abline(h = 2, col = "red")

#Cut tree into clusters/groups
cutree(hc2, h = 2)
```
```{r}
# Try K instead of h
hc2 <- hclust(dist(y))
plot(hc2)
abline(h = 2, col = "red")
grps <- cutree(hc2, k = 3)
grps
```


Q. How does this compare to your known 'col' groups?
plot the data colored by their `hclust()` result with k = 3
```{r}
plot(y, col = grps)
```


How many points in each cluster? 
```{r}
table(grps)
```


Cross - tabulate i.e. compare your clustering result with the known answer!
```{r}
table(grps, col)
```





## PCA 
### Ex/ Gene Expression 
```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV", row.names=1)

head(mydata)

dim(mydata)
nrow(mydata)
#samples are columns, genes are rows
```
There are 100 genes



```{r}
# Let's do prcomp()

pca <- prcomp(t(mydata), scale = TRUE)
attributes(pca)
```
Gonna focus on "x" -> `pca$x`

```{r}
# PC1 vs. PC2
plot(pca$x[,1], pca$x[,2])
```

```{r}
#Variance captured per PC. Present variance is often more informative to look at
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```



### NOW let's make a Scree Plot
```{r}
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
```


Make Color vector
```{r}
# A vector of colors for wt and ko samples
plot(pca$x[,1], pca$x[,2], 
     col = c("red", "red", "red", "red", "red", "blue", "blue", "blue", "blue", "blue"))
```


### A more "elegant" code for this ^
```{r}
# A vector of colors for wt and ko samples
colvec <- colnames(mydata) 
colvec[grep("wt", colvec)] <- "red" 
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16, 
     xlab=paste0("PC1 (", pca.var.per[1], "%)"), 
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))


identify(pca$x[,1], pca$x[,2], labels=colnames(mydata))
text(pca$x[,1], pca$x[,2], labels = colnames(mydata), pos=c(rep(4,5), rep(2,5)))
```









# Lecture 8 Lab 
## Part 1
```{r}
food <- read.csv("UK_foods.csv")
head(food)
```

Questions
```{r}
# Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

dim(food)
nrow(food)
ncol(food)

# 17 rows and 5 columns. R functions: dim(), nrow(), ncol(). 
```

```{r}
food <- read.csv("UK_foods.csv", row.names = 1)
head(food)
```


```{r}
dim(food)
```


Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?
- I like the first version over the "x <- x[,-1]" version because if you kept doing this it would take away the first columns. Therefore DANGEROUS. 

##Spotting major differences and trends
A cursory glance over the numbers in this table does not reveal much of anything. Indeed in general it is difficult to extract meaning in regard to major differences and trends from any given array of numbers. Generating regular bar-plots and various pairwise plots does not help too much either:

```{r}
barplot(as.matrix(food), beside=T, col=rainbow(nrow(food)))
```

```{r}
# Q3: Changing what optional argument in the above barplot() function results in the following plot?
barplot(as.matrix(food), beside=F, col=rainbow(nrow(food)))

```

*NO Q4*

```{r}
# Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?
pairs(food, col=rainbow(10), pch=16)

# Each country is pairwised with each food group. 
# Ex/ First Row: England on y axis and other three countries on x. A linear graph shows similarities. So England is similar to Wales and Scotland 
# this isn't useful if you're using several data points. 
```

Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?
- eat it's more food...

## PCA to the rescue
```{r}
pcaUK <- prcomp( t(food) )
summary(pcaUK)
```
--> Proportion of Variance shows us true variance. How PC1 is the biggest!

```{r}
# Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

plot(pcaUK$x[,1], pcaUK$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
```


```{r}
# Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

plot(pcaUK$x[,1], pcaUK$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pcaUK$x[,1], pcaUK$x[,2], colnames(food), col = c("yellow", "red", "blue", "green3"))
```

## Calculating how much variation in the original data each PC accounts for.
```{r}
v <- round( pcaUK$sdev^2/sum(pcaUK$sdev^2) * 100 )
v
```

```{r}
z <- summary(pcaUK)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

## Digging deeper (variable loadings)
We can also consider the influence of each of the original variables upon the principal components (typically known as loading scores). This information can be obtained from the prcomp() returned $rotation component. It can also be summarized with a call to biplot(), see below:

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pcaUK$rotation[,1], las=2 )
```

Here we see observations (foods) with the largest positive loading scores that effectively “push” N. Ireland to right positive side of the plot (including Fresh_potatoes and Soft_drinks).

We can also see the observations/foods with high negative scores that push the other countries to the left side of the plot (including Fresh_fruit and Alcoholic_drinks).

```{r}
# Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

par(mar=c(10, 3, 0.35, 0))
barplot( pcaUK$rotation[,2], las=2 )

# Fresh_potatoes and soft_drinks. they show us that in PC 2, fresh_potatoes pushes Scotland and Wales variance. Wales like their portatoes, and Scotland like their soda. 
```



## Part 2 PCA for RNA-seq data
RNA-seq results often contain a PCA (or related MDS plot). Usually we use these graphs to verify that the control samples cluster together. However, there’s a lot more going on, and if you are willing to dive in, you can extract a lot more information from these plots. The good news is that PCA only sounds complicated. Conceptually, as we have hopefully demonstrated here and in the lecture, it is readily accessible and understandable.

```{r}
rna.data <- read.csv("expression.csv", row.names=1)
head(rna.data)
```
genes = row, samples = columns 

```{r}
# Q10: How many genes and samples are in this data?

dim(rna.data)
nrow(rna.data)
ncol(rna.data)
```

REST LOOK ABOVE --> DID IN CLASS

Additional stuff:
```{r}
## Another way to color by sample type
## Extract the first 2 characters of the sample name
sample.type <- substr(colnames(rna.data),1,2)
sample.type
```
```{r}
## now use this as a factor input to color our plot
plot(pca$x[,1], pca$x[,2], col=as.factor(sample.type), pch=16)
```

```{r}
loading_scores <- pca$rotation[,1]

## Find the top 10 measurements (genes) that contribute
## most to PC1 in either direction (+ or -)
gene_scores <- abs(loading_scores) 
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)

## show the names of the top 10 genes
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes 
```

